{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Input,Dense,Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D,LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the text data\n",
    "Iterate over the folders in which our text documents are stored, and format them into a list of documents. \n",
    "At the same time also prepare a list of class indices matching the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\INSOFE\\7321C\\20190728_Batch60_CSE7321c_RNN_Students_Copy\\20190728_Batch60_CSE7321c_RNN_Students_Copy\n",
      "['.DS_Store', 'kalam', 'obama', 'romney']\n"
     ]
    }
   ],
   "source": [
    "docs = []          # list of text samples\n",
    "labels = []        # list of label ids\n",
    "labels_Index = {}  # dictionary mapping label index to label name\n",
    "\n",
    "PATH = os.getcwd()\n",
    "print(PATH)\n",
    "\n",
    "TEXT_DATA_DIR = os.path.join(PATH, \"txt\")\n",
    "\n",
    "TEXT_DATA_DIR_LIST = os.listdir(TEXT_DATA_DIR)\n",
    "\n",
    "print(TEXT_DATA_DIR_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR_LIST = TEXT_DATA_DIR_LIST[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalam dir has following docs ['doc01.txt', 'doc02.txt', 'doc03.txt', 'doc04.txt', 'doc05.txt', 'doc06.txt', 'doc07.txt', 'doc08.txt', 'doc09.txt', 'doc10.txt', 'doc11.txt', 'doc12.txt'] \n",
      "\n",
      "obama dir has following docs ['obama01.txt', 'obama02.txt', 'obama03.txt', 'obama04.txt', 'obama05.txt', 'obama06.txt', 'obama07.txt', 'obama08.txt', 'obama09.txt', 'obama10.txt', 'obama11.txt', 'obama12.txt'] \n",
      "\n",
      "romney dir has following docs ['romney01.txt', 'romney02.txt', 'romney03.txt', 'romney04.txt', 'romney05.txt', 'romney06.txt', 'romney07.txt', 'romney08.txt', 'romney09.txt', 'romney10.txt', 'romney11.txt', 'romney12.txt'] \n",
      "\n",
      "36 docs with labels -->  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "for name in TEXT_DATA_DIR_LIST:\n",
    "    \n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    \n",
    "    if os.path.isdir(path):\n",
    "        \n",
    "        label_Id = len(labels_Index)\n",
    "        labels_Index[label_Id] = name\n",
    "        \n",
    "        print(\"{} dir has following docs {} \\n\".format( name, os.listdir(path) ))\n",
    "        \n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            \n",
    "            fpath = os.path.join(path, fname)\n",
    "            f = open(fpath, encoding = \"ISO-8859-1\")\n",
    "        \n",
    "            t = f.read()\n",
    "            \n",
    "            docs.append(t)\n",
    "            \n",
    "            f.close()\n",
    "            \n",
    "            labels.append(label_Id)\n",
    "            \n",
    "            \n",
    "print(len(labels), 'docs with labels --> ', labels)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the text samples and labels into tensors that can be fed into a neural network. \n",
    "\n",
    "To do this, we will rely on Keras utilities \n",
    "\n",
    "    keras.preprocessing.text.Tokenizer \n",
    "    keras.preprocessing.sequence.pad_sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenizer__\n",
    "\n",
    "    Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "\n",
    "__fit_on_texts(texts)__\n",
    "\n",
    "    Arguments:  \n",
    "        texts: list of texts to train on.\n",
    "        \n",
    "__word_index__ attribute: \n",
    "\n",
    "    Dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7314 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# Prepare tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "word_Index=tokenizer.word_index\n",
    "\n",
    "vocab_Size=len(word_Index)+1\n",
    "print('Found %s unique tokens.' % vocab_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__texts_to_sequences(texts)__\n",
    "\n",
    "    Arguments:\n",
    "        texts: list of texts to turn to sequences.\n",
    "    Return: list of sequences (one per text input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research missions for doubling the food production\n",
      "\n",
      "Technology is a Nonlinear Tool for Economic Growth\n",
      "\n",
      "I am delighted to address and interact with the Scientists of Gujarat Life Science Centre, Vadodara and also the farmers of Gujarat. Gujarat Life Sciences through the science Ashram with the mission of providing new generation of agriculture biotech products to lead to eco- agriculture farming in various parts of India and other countries. I also greet Gujarat Scientists and farmers of Gujarat Life Sciences with partnership State of Bihar has created world record in potato production. Let me share some thoughts on the topic Research missions for doubling the food production. \n",
      "Friends let me first begin with my experience with Bihar farmers.\n",
      "\n",
      "Agriculture mission\n",
      "\n",
      "I have been continuously in contact with Paliganj farmers of Bihar where Technology Information Forecasting and Assessment Council (TIFAC) had a mission of doubling the productivity of rice and wheat in areas near RP Channel-5 in Bihar using innovative integrated farming and marketing methods. These results have spread to many areas through people?s efforts and are applicable to the whole of Bihar, Eastern Uttar Pradesh and other areas which have similar agro-climatic conditions. These regions could be transformed into the granaries of India. When I visited Paliganj in May 2011 and discussed with Paliganj farmers on how to enhance the productivity and what is the role of farmers in different districts of Bihar, many suggestions came from the farmers. One of the very powerful suggestion from a farmer Shri Deodhar Sharma was \"we farmers do not need any subsidy for farming, what we need is good and assured quality farm inputs such as seeds, fertilizers, pesticides and above all these inputs should come to the farmers at the right time for right farming from certified suppliers. Also, we need the best of technology, best of irrigation methods in difficult times and uninterrupted supply of electric power\".\n",
      "\n",
      "\n",
      "With this background, I organized four farmers from Paliganj area to visit Gujarat where agricultural sector is growing at 9% per annum. A course was organized in Indian Institute of Management, Ahmedabad for the Paliganj farmers on agricultural management which had an impact in agricultural renaissance across the nation. I also arranged the farmers to visit Amalsad, Chikku Fruit Cooperative and Warana PURA in Maharashtra so that the farmers can understand the merits of the cooperative system. These farmers have gone back to Paliganj with a promise that they will create a new model of technology based cooperative system for agriculture which will be a unique model in the country. I am sharing this experience of Paliganj farmer?s, since it has the potential to spread across the country in different agriculture, horticulture and floriculture areas. Since, there are over 607 Krishi Vigyan Kendras in the country, I would suggest that the scientists and field engineers deployed in these Krishi Vigyan Kendras to work closely with such mission oriented farmers and spread the development message to the entire district, so that the overall productivity of the district grows at a fast rate. Such an intensive intervention is needed in Bihar and Uttar Pradesh which have a tremendous potential to get transformed as grain granaries like Punjab and Haryana.\n",
      "\n",
      "Rural Innovation System\n",
      "\n",
      "I would like to now talk about excellent work done by Dr. DN Tewari in Jatropha area. Under his leadership a NGO has worked near Allahabad and converted 735 hectares of waste land (infertile, marginal and abandoned land) into Jatropha producing land leading to earning of rupees fifty thousand per hectare. Historically, this land was given to the farmers of Allahabad district which was alkaline in nature. Many of the farmers started using the mud from the land for producing bricks. The others abandoned and went to cities for job. When the special characteristics of jatropha was realized through the work of Dr. Tewari, the villagers undertook the cultivation of in that land. After this plantation, these villagers have realized energy independence through the use of bio-fuel. The villagers do not use kerosene for cooking or petrol-diesel for running their generators and vehicles. They only use biofuel for running their vehicles and methane generated from jatropha cake for house hold cooking. Non-use of kerosene has resulted in better health for all the rural women in that area. Jatropha cultivation has also been used as a heat shield for banana plantation in fertile land during summer. Because of the use of jatropha, the soil has become neutral from being alkaline. Also, use of jatropha has reduced the environmental pollution. Looking at the success of 735 farmers, over 10,000 farmers have taken up plantation of jatropha in Allahabad region. In addition to soil upgradation, jatropha plantation has enabled water retention and enhancement of water table in the neighbouring areas. Also, I noticed a significant difference in those villages from environmental pollution angle. I am giving these examples of excellence in agriculture to this enlightened audience, so that scientists can have a project through a consultant for documenting all the special agricultural work of excellence either in seeds, use of fertilizer, organic farming, environmental up-gradation, productivity enhancement in all 131 agro-climatic zones of the country. This can become an augmented material for integrated rural developmental programme in the country.\n",
      "\n",
      "Enhancing the productivity of bottom of the pyramid farmers\n",
      "\n",
      "Friends, you all scientists has to carved out a task for developing vision 2020 in consultation with stakeholders for prioritizing the 12th plan programmes. The emphasize has set to be in enhancing both productivity and profitability of farming in all its dimensions including climate resistance agriculture and development of quality human resource for implementing the plan schemes.\n",
      "\n",
      "\n",
      "My study of Indian agriculture indicates that 50% of the agricultural workforce is deployed in small and marginal holding and many of them produce paddy or wheat less than 1 to 1.5 tonne per hectare. The benefits of high-technology agriculture reach largely to the medium and rich farmers, including the loans given by the bank. There is a need for addressing the technological and physical inputs needed by the bottom of the pyramid segment, so that we can remove poverty from the farming community as a whole. This can be done by asking the following questions:\n",
      "a)   The varieties of food-grains which these farmers who are at the bottom are using, \n",
      "\n",
      "b)   How many of the \"released\" varieties are actually needed for this segment\n",
      "\n",
      "c)   Designing demonstration project which can double the yields in two years, since it is not difficult to empower farmer who is producing 1 ton /ha to produce 2 tons, whereas it is difficult to do that (double) at the top where the production is already 7 or 8 tonnes. \n",
      "\n",
      "d)   What are the simple methods which are already there and can be popularized among the farmers of this segment. \n",
      "\n",
      "e)   Tools required to reduce the drudgery of farming and improving the efficiency\n",
      "\n",
      "f)   Enhancing the water productivity of these farmers, who might be having their own small bore well or water pond.\n",
      "\n",
      "\n",
      "Such an intensive programme can be launched by the scientists which can increase the productivity of nearly 60% of the total agricultural farm holders. Since it is in the bottom of the pyramid, we have to work for productivity of land, water and labour, all at the same time. Aim should be to minimize the input and maximize the output.\n",
      "\n",
      "TN State specific Precision Farming Project\n",
      "\n",
      "Precision Farming or Precision Agriculture is a technology adopted in Tamilnadu is doing the right thing, in the right place and at the right time has created a great awareness among the farmers because it has given higher productivity, market access and made farmers as entrepreneurs. In this process, the collected information may be used more precisely tp evaluate optimum sowing density, estimate fertilizers and other inputs needs, and to more accurately predict crop yields. It helps in avoiding unwanted practices to a crop, regardless of local soil/climate conditions, i.e., it reduces labour, water, inputs such as fertilizers, pesticides etc. and assures quality produce. Precision Farming Project was started in Tamil Nadu first in Dharmapuri district during 2004-05. It was implemented initially in 250 acres in 2004-05, 500 acres in 2005-06 and 250 acres in 2006-07. The first crop has been taken up under the total guidance of Scientists from the University. Subsequent 5 crops had been taken up by the farmers in 3 years.\n",
      "\n",
      "\n",
      "The processes were given deep thought and the products matched with the expected quality. The cluster members were brought under an association called ?Precision Farmers Association? registered under societies act. The process has finally led to the establishment of a farmer owned producer Company called \"Dharmapuri Precision Farmers? Agro Services Ltd\", registered under Company act and this is the first Producer Company in the Tamil Nadu state where 166 farmers of Dharmapuri farmers are share holders. Later the second producer company owned by the farmers ?Erode Precision Farm Producers Company Ltd? was established .\n",
      "\n",
      "\n",
      "The result of this experience has multiple effects, it has not only focused on maximizing the productivity and enhance profitability but to empower the farmers socially, economically and technically. The focus was much on social capital through strengthening cluster level associations and district level Ltd company. For example, during one of the Annual Farmers? Day meeting held at Salem, one of the precision farmers made a statement during his speech with tears in his eyes that for the first time, his family was able to see the ?one lakh currency? bundles. It had attracted the attention of policy makers, bankers and insurance firms. The precision farmers have doubled the yield in 45 crops compared to National and State Average. I would suggest the scientists to study such success stories, so that they can carry out further research on programme management and implement them in different areas in the country.\n",
      "\n",
      "Gujarat Experience in 9% agriculture growth\n",
      "\n",
      "On 12 Oct 2009, when I was in the middle of the academic session, as a part of the course GRIIT-II at IIM, Ahmadabad, there was a call from Centre for Management in Agriculture (CMA) that they would like to have a session with me. I agreed and met the CMA team comprises of its Dean, Professors, faculties and research scholars at IIM Ahmadabad. From CMA side, Dean, Prof Sukpaul singh, Dr Ravindra Dholakia and other were participating. After my talk I asked one question to CMA Team. \"Can you please explain, when the entire India has achieved only 3% growth rate in Agriculture production, how Gujarat is able to attain 9% growth rate?\" In a simple way, the entire CMA team explained the growth story, which is an eye opener for the entire nation.\n",
      "\n",
      "\n",
      "The major factors that have impeded the agricultural productivity in the state of Gujarat in the early 2000 were the following:\n",
      "a)  Depletion of water table \n",
      "b)   Deterioration of soil and water conditions due to salinity ingress \n",
      "c)  Irregularity of rainfall \n",
      "d)   Recurrent droughts\n",
      "\n",
      "\n",
      "This was the situation prevailing 6 years before, but what made phenomenal transformation in Gujarat Agriculture sector that resulted into 9% growth in agriculture and maintaining and sustaining it for the last 7 years is the success story. Efforts were made for propagation of scientific agriculture and water conservation to make agriculture stable sustainable with increase in productivity and income.\n",
      "\n",
      "\n",
      "In Sep 2003, Minister for Agriculture, Government of Gujarat called the CMA team for discussion to attend an urgent meeting. The agenda was how to manage the situation of agriculture growth during that time in Gujarat. That period, Gujarat government has expected a surplus produce and increased productivity due to excellent weather conditions and also the based on the other favourable conditions for the growth. Minister of Agriculture wanted to seek the suggestion from CMA team, how to manage the excess food production, if it is not managed well, it may lead to sudden drop in prices and farmers may not get their revenue and that will inflict heavy losses and not able to enjoy the benefits of the agricultural growth. CMA team had worked out a short term, medium term and long term measures and several of those suggestions were accepted and implemented by liberalizing the export commodities and other measures have yielded the results and that year it was witnessed the 40% growth rate. Since then Gujarat had witnessed the continuous growth rate of 7 to 9% over the past 6 years. I asked them how it was possible and what are the favourable conditions for this sustainable growth in agriculture for the past 6 years? It is a unique phenomenon when the entire nation is struggling to achieve even 3% growth?\n",
      "\n",
      "\n",
      "This has been achieved by the mission mode action by the Gujarat Government. Gujarat government had determined not to give free power to farmers instead ensured 3 phase electrical supply to agriculture on 24/7 basis, by reorganizing the power distribution channels with latest state-of-the art gadgets installed and segregating the power distribution between domestic consumption and agricultural consumption. This single action with creating a vigilance force with ex-servicemen to track the power theft and pilferage had resulted into reduced power theft and increased availability of power to farmers.\n",
      "\n",
      "\n",
      "Micro irrigation facilities were established on a wider scale across the state in a mission which brought 20,000 hectares under the micro irrigation scheme. Due to the construction of check dams, even those areas that face perennial water scarcity have seen an increase in the water table by 3-12 m. Within a period of three years alone, over 75,000 of these structures were created. Four centre of excellence has been created with agriculture universities.\n",
      "\n",
      "\n",
      "These experiences clearly shows that \"We can do it\", but what we need is a mission mode approach across the nation with respect to Agriculture; and we need a creative leadership who has a vision for leading the second green revolution for the next 10 years like the First Green Revolution Pioneer Mr. C. Subramanian, based on our successful experience in the field of agriculture and agro food processing, which leads to marketing in India for accelerating the growth and increase the productivity to achieve 10% growth rate in Agriculture which is certainly possible in India.\n",
      "\n",
      "Missions for agriculture scientists during 2012 Â– 2020\n",
      "\n",
      "Based on my experience, I would like to suggest the following research missions for the agriculture scientists:\n",
      "a) \tDevelopment of farm practices, input material and mechanization system for small and marginal farmers of the nation. \n",
      "\n",
      "b) \tAnalyzing the micro-characteristics of fallow land (25 million hectares) barren and unculturable land (17 million hectares) and culturable waste land (13 million hectares) and suggest different plantations based on agro-climatic and soil conditions to at-least retrieve 50% of this land in the next 10 years. \n",
      "\n",
      "c) \tMapping the 3 million hectares of water bodies (in 330 million hectares of land) consisting of ponds, reservoirs, brackish-water, lagoons, rivers and canals in partnership with NRSA (National Remote Sensing Agency) and suggest methods based on research for rejuvenating these water bodies and develop methods for enhancing the overall area underwater by 4 million hectares by 2020. \n",
      "\n",
      "d) \tThe excessive of use of chemical fertilizers and pesticides is said to have polluted the ground water and also enhanced the toxicity in the farm produce. There is a need to quantify the pollution in water and farm produce through research, so that suitable strategies can be developed for progressive arresting this tendency which is vital for continuous good health of the nation. \n",
      "\n",
      "e) \tIn our coastal area, Indian National Centre for Ocean Information services is using the satellite data of the ocean temperature and colour (chlorophyll), twice a week is able to establish the potential fishing zone in different parts of the Indian coast line. This information is sent to each landing station in the coast which details about where the potential fishing zone, its distance from the coast, the bearing in which one has to go and the depth of the zone is displayed for the information of the fishermen. Scientists have work with Indian National Centre for Ocean Information Services, so that data is further refind and provided to all the fishermen irrespective of what type of boats they have.\n",
      "\n",
      "\n",
      "f) \tThere is a need to carry out research on how the small and marginal farmers can come together through the intervention of Krishi Vikas Kendra and form rural cooperatives in their areas so that integrated farming including agro-processing can be carried out which will lead to value added products and enhance the revenue earning ability of bottom of the pyramid farmers. For this purpose, one or two model cooperatives per district can be created by scientists in the 12th plan period. \n",
      "\n",
      "g) \tThe stakeholders in agriculture include farmers, agricultural scientists, \n",
      "\n",
      "meteorologists, agricultural planners, seed bank, water and irrigation system managers, organic and inorganic fertilizers manufacturers, chemical and bio-pesticides manufacturers, farm equipment lending agencies, co-operative banking system and financial institutions, warehouses and godowns, procurement agencies, distribution system and the coordinating ministries from the Central and State Governments. The success of the mission is totally dependent on the synchronized integrated action among all the stakeholders through meticulous integrated planning, funding, scheduling, storing, agro-processing and marketing. Scientists can develop a self-coordinating mechanism for achieving this goal as an organizational development mission. \n",
      "\n",
      "h) \tBringing out a document of all the agricultural technologies and research output available applicable in the present agro-climatic context in the country by different non-governmental agencies, placing them on a website and updating them periodically.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Friends, I have seen three dreams which have taken shape as vision, mission and realization. Space programme of ISRO (Indian Space Research Organization), AGNI programme of DRDO (Defence Research and Development Organization) and PURA (Providing Urban Amenities in Rural Areas) becoming the National Mission. Of course, these three programmes succeeded in the midst of many challenges and problems. I have worked in all these three areas. I want to convey to you what I have learnt on leadership from these three programmes: \n",
      "a. \tLeader must have a vision. \n",
      "b. \tLeader must have passion to realize the vision. \n",
      "c. \tLeader must be able to travel into an unexplored path. \n",
      "d. \tLeader must know how to manage a success and failure. \n",
      "e. \tLeader must have courage to take decisions. \n",
      "f. \tLeader should have nobility in management.\n",
      "g. \tLeader should be transparent in every action. \n",
      "h. \tLeader must work with integrity and succeed with integrity.\n",
      "\n",
      "\n",
      "For success in all your missions you have to become creative leaders. Creative leadership means exercising the vision to change the traditional role from the commander to the coach, manager to mentor, from director to delegator and from one who demands respect to one who facilitates self-respect. I am sure, Gujarat Life Sciences is getting equipped such leaders who will have the mission of making Indian farmers proud of their profession. \n",
      "\n",
      "My best wishes to all of you. \n",
      "May God Bless you.\n",
      "\n",
      " [115, 545, 9, 2341, 1, 512, 829, 117, 10, 5, 4276, 1449, 9, 263, 285, 8, 72, 1152, 3, 1034, 2, 1417, 17, 1, 291, 4, 532, 85, 44, 312, 1631, 2, 98, 1, 195, 4, 532, 532, 85, 372, 102, 1, 44, 4277, 17, 1, 149, 4, 724, 60, 344, 4, 273, 4278, 560, 3, 163, 3, 1450, 273, 830, 6, 712, 825, 4, 111, 2, 113, 561, 8, 98, 1637, 532, 291, 2, 195, 4, 532, 85, 372, 17, 1451, 164, 4, 130, 31, 444, 58, 590, 6, 4279, 829, 112, 54, 283, 165, 678, 19, 1, 949, 115, 545, 9, 2341, 1, 512, 829, 79, 112, 54, 108, 617, 17, 26, 358, 17, 130, 195, 273, 149, 8, 18, 68, 1177, 6, 3014, 17, 1304, 195, 4, 130, 75, 117, 299, 2342, 2, 2947, 1929, 4280, 80, 5, 149, 4, 2341, 1, 546, 4, 1930, 2, 1452, 6, 191, 1305, 4281, 4282, 303, 6, 130, 235, 1179, 534, 830, 2, 1453, 1454, 73, 631, 18, 970, 3, 76, 191, 102, 46, 198, 632, 2, 20, 3015, 3, 1, 816, 4, 130, 2343, 1931, 1157, 2, 113, 191, 64, 18, 1172, 1180, 1932, 774, 73, 3016, 174, 21, 3017, 94, 1, 3018, 4, 111, 40, 8, 515, 1304, 6, 128, 1668, 2, 3019, 17, 1304, 195, 19, 67, 3, 1055, 1, 546, 2, 37, 10, 1, 459, 4, 195, 6, 255, 891, 4, 130, 76, 1933, 322, 27, 1, 195, 41, 4, 1, 197, 1058, 2344, 27, 5, 1934, 971, 4283, 4284, 33, 11, 195, 52, 24, 82, 173, 4285, 9, 830, 37, 11, 82, 10, 122, 2, 1935, 438, 972, 1455, 150, 23, 3020, 1669, 1936, 2, 585, 32, 73, 1455, 88, 114, 3, 1, 195, 38, 1, 201, 57, 9, 201, 830, 27, 4286, 4287, 98, 11, 82, 1, 118, 4, 117, 118, 4, 1456, 1454, 6, 591, 387, 2, 4288, 2345, 4, 892, 227, 17, 16, 2346, 8, 1457, 125, 195, 27, 1304, 633, 3, 1062, 532, 75, 634, 679, 10, 1063, 38, 403, 893, 4289, 5, 244, 33, 1457, 6, 320, 1937, 4, 508, 2347, 9, 1, 1304, 195, 19, 634, 508, 64, 80, 43, 1938, 6, 634, 4290, 162, 1, 51, 8, 98, 3021, 1, 195, 3, 1062, 4291, 4292, 2348, 2349, 2, 4293, 562, 6, 4294, 47, 7, 1, 195, 29, 336, 1, 4295, 4, 1, 2349, 103, 73, 195, 18, 814, 129, 3, 1304, 17, 5, 188, 7, 35, 14, 288, 5, 60, 415, 4, 117, 276, 2349, 103, 9, 273, 64, 14, 21, 5, 146, 415, 6, 1, 69, 8, 72, 1418, 16, 358, 4, 1304, 1934, 198, 373, 15, 31, 1, 432, 3, 970, 162, 1, 69, 6, 255, 273, 4296, 2, 4297, 191, 373, 59, 20, 147, 4298, 2350, 2351, 3022, 6, 1, 69, 8, 56, 761, 7, 1, 291, 2, 516, 3023, 3024, 6, 73, 2350, 2351, 3022, 3, 50, 1670, 17, 150, 149, 2352, 195, 2, 970, 1, 106, 629, 3, 1, 775, 776, 47, 7, 1, 2353, 546, 4, 1, 776, 1939, 38, 5, 1458, 777, 150, 43, 1671, 1459, 10, 725, 6, 130, 2, 1931, 1157, 64, 18, 5, 1037, 432, 3, 119, 3017, 23, 3025, 3018, 63, 4299, 2, 4300, 245, 356, 103, 8, 56, 63, 3, 96, 342, 53, 3026, 50, 374, 25, 458, 4301, 3027, 6, 1064, 633, 265, 49, 218, 5, 4302, 31, 404, 1305, 2354, 2, 4303, 3028, 1181, 4, 1065, 405, 4304, 1940, 2, 1941, 405, 94, 1064, 2355, 405, 439, 3, 2356, 4, 4305, 3029, 1672, 893, 3030, 4306, 16, 405, 33, 333, 3, 1, 195, 4, 2354, 776, 64, 33, 3031, 6, 1942, 76, 4, 1, 195, 450, 235, 1, 4307, 27, 1, 405, 9, 2355, 4308, 1, 385, 1941, 2, 412, 3, 894, 9, 166, 40, 1, 890, 1943, 4, 1064, 33, 1460, 102, 1, 50, 4, 458, 3027, 1, 2357, 4309, 1, 3032, 4, 6, 7, 405, 126, 16, 1944, 73, 2357, 18, 1460, 142, 668, 102, 1, 290, 4, 388, 1066, 1, 2357, 52, 24, 290, 3033, 9, 2358, 39, 3034, 3035, 9, 726, 34, 3036, 2, 3037, 35, 127, 290, 4310, 9, 726, 34, 3037, 2, 3038, 1299, 27, 1064, 4311, 9, 451, 727, 2358, 951, 290, 4, 3033, 31, 1461, 6, 92, 238, 9, 32, 1, 245, 192, 6, 7, 633, 1064, 3032, 31, 98, 68, 460, 23, 5, 2359, 4312, 9, 4313, 1944, 6, 4314, 405, 249, 3039, 66, 4, 1, 290, 4, 1064, 1, 1165, 31, 210, 3040, 27, 282, 3031, 98, 290, 4, 1064, 31, 1462, 1, 1673, 1067, 818, 38, 1, 237, 4, 3028, 195, 147, 284, 347, 195, 18, 757, 62, 1944, 4, 1064, 6, 2354, 778, 6, 1639, 3, 1165, 4315, 1064, 1944, 31, 4316, 251, 4317, 2, 3041, 4, 251, 1463, 6, 1, 4318, 191, 98, 8, 2360, 5, 4319, 895, 6, 86, 592, 27, 1673, 1067, 4320, 8, 72, 715, 73, 2361, 4, 509, 6, 273, 3, 16, 1891, 1306, 47, 7, 291, 29, 18, 5, 680, 102, 5, 4321, 9, 4322, 32, 1, 890, 634, 50, 4, 509, 1464, 6, 3020, 290, 4, 3042, 3043, 830, 1673, 62, 4323, 546, 3041, 6, 32, 4324, 1180, 1932, 4325, 4, 1, 69, 16, 29, 210, 43, 4326, 491, 9, 534, 245, 3044, 582, 6, 1, 69, 1156, 1, 546, 4, 1068, 4, 1, 896, 195, 79, 13, 32, 291, 31, 3, 4327, 74, 5, 973, 9, 544, 318, 720, 6, 4328, 17, 2362, 9, 4329, 1, 3045, 304, 1674, 1, 3046, 31, 627, 3, 21, 6, 1156, 389, 546, 2, 2363, 4, 830, 6, 32, 136, 1307, 625, 1182, 3047, 273, 2, 106, 4, 438, 159, 1176, 9, 2364, 1, 304, 4330, 26, 593, 4, 320, 273, 4331, 7, 1069, 4, 1, 634, 4332, 10, 3024, 6, 135, 2, 1940, 3048, 2, 76, 4, 84, 831, 4333, 39, 1452, 233, 77, 264, 3, 264, 303, 4334, 893, 3030, 1, 465, 4, 243, 117, 273, 563, 4335, 3, 1, 1308, 2, 897, 195, 625, 1, 1945, 333, 25, 1, 3049, 59, 10, 5, 82, 9, 2365, 1, 898, 2, 1465, 1455, 725, 25, 1, 1068, 4, 1, 896, 2366, 47, 7, 11, 29, 1675, 323, 27, 1, 830, 331, 23, 5, 816, 16, 29, 21, 374, 25, 628, 1, 974, 630, 5, 1, 3050, 4, 512, 2367, 64, 73, 195, 22, 20, 38, 1, 1068, 20, 235, 763, 67, 76, 4, 1, 4336, 3050, 20, 1070, 725, 9, 16, 2366, 1071, 3051, 4337, 680, 64, 29, 1676, 1, 3052, 6, 133, 65, 373, 15, 10, 24, 591, 3, 1072, 1934, 22, 10, 2355, 264, 4338, 4339, 3, 831, 301, 2368, 3053, 15, 10, 591, 3, 52, 7, 1676, 38, 1, 1045, 75, 1, 829, 10, 547, 402, 39, 490, 1946, 899, 37, 20, 1, 900, 1454, 64, 20, 547, 59, 2, 29, 21, 4340, 417, 1, 195, 4, 16, 2366, 667, 770, 1053, 3, 1309, 1, 4341, 4, 830, 2, 1636, 1, 779, 1466, 1156, 1, 251, 546, 4, 73, 195, 22, 433, 21, 594, 34, 182, 135, 4342, 286, 39, 251, 4343, 150, 43, 1671, 582, 29, 21, 1947, 25, 1, 291, 64, 29, 832, 1, 546, 4, 447, 2369, 4, 1, 1948, 634, 972, 3054, 373, 15, 10, 6, 1, 1068, 4, 1, 896, 11, 18, 3, 50, 9, 546, 4, 405, 251, 2, 3055, 32, 38, 1, 178, 57, 975, 88, 21, 3, 3056, 1, 2370, 2, 4344, 1, 1949, 2371, 164, 1950, 1073, 830, 680, 1073, 830, 39, 1073, 273, 10, 5, 117, 3057, 6, 2372, 10, 346, 1, 201, 463, 6, 1, 201, 252, 2, 38, 1, 201, 57, 31, 444, 5, 90, 4345, 417, 1, 195, 66, 15, 31, 333, 419, 546, 901, 1074, 2, 170, 195, 23, 780, 6, 16, 722, 1, 4346, 299, 128, 21, 460, 45, 3058, 4347, 2373, 4348, 4349, 3059, 4350, 1669, 2, 113, 1455, 357, 2, 3, 45, 4351, 3060, 2374, 3052, 15, 1183, 6, 4352, 3061, 1033, 3, 5, 2374, 3062, 4, 1184, 1165, 1182, 774, 8, 667, 15, 4353, 3055, 251, 1455, 150, 23, 1669, 1936, 4354, 2, 4355, 438, 831, 1073, 830, 680, 33, 450, 6, 2375, 3063, 108, 6, 2376, 776, 249, 3064, 3065, 15, 33, 3066, 4356, 6, 2377, 2378, 6, 3064, 3065, 976, 2378, 6, 2379, 4357, 2, 2377, 2378, 6, 1951, 4358, 1, 108, 2374, 31, 68, 757, 62, 265, 1, 1948, 1677, 4, 291, 27, 1, 1952, 4359, 303, 2380, 80, 68, 757, 62, 25, 1, 195, 6, 232, 65, 1, 4360, 95, 333, 1044, 375, 2, 1, 560, 2381, 17, 1, 1185, 438, 1, 2382, 313, 95, 635, 265, 43, 902, 298, 1073, 195, 902, 3067, 265, 1678, 676, 1, 722, 31, 348, 537, 3, 1, 3068, 4, 5, 1934, 1953, 2383, 517, 298, 2376, 1073, 195, 1180, 833, 2384, 3067, 265, 517, 676, 2, 16, 10, 1, 108, 2383, 517, 6, 1, 2375, 3063, 164, 75, 4361, 195, 4, 2376, 195, 20, 283, 3054, 903, 1, 422, 2383, 517, 1953, 25, 1, 195, 4362, 1073, 972, 3069, 517, 2384, 33, 1679, 1, 1186, 4, 16, 358, 31, 595, 2385, 15, 31, 24, 127, 2386, 19, 4363, 1, 546, 2, 1055, 2363, 28, 3, 1072, 1, 195, 3070, 1467, 2, 4364, 1, 636, 33, 211, 19, 637, 950, 102, 3071, 2382, 728, 4365, 2, 776, 728, 2384, 517, 9, 619, 249, 41, 4, 1, 4366, 195, 120, 729, 1187, 38, 4367, 41, 4, 1, 1073, 195, 170, 5, 2387, 249, 49, 1954, 17, 1680, 6, 49, 1468, 7, 9, 1, 108, 57, 49, 208, 33, 406, 3, 184, 1, 41, 3072, 3073, 4368, 15, 80, 3074, 1, 2388, 4, 781, 3075, 3076, 2, 834, 3077, 1, 1073, 195, 18, 1955, 1, 3078, 6, 1956, 2380, 2389, 3, 175, 2, 164, 1663, 8, 56, 761, 1, 291, 3, 593, 150, 237, 584, 47, 7, 35, 29, 2390, 74, 681, 115, 19, 582, 508, 2, 3079, 84, 6, 255, 191, 6, 1, 69, 532, 358, 6, 403, 273, 285, 19, 760, 2391, 3080, 40, 8, 33, 6, 1, 132, 4, 1, 1633, 3081, 23, 5, 441, 4, 1, 244, 4369, 1957, 38, 3082, 3083, 59, 33, 5, 564, 27, 312, 9, 508, 6, 273, 1188, 7, 35, 56, 63, 3, 18, 5, 3081, 17, 54, 8, 4370, 2, 246, 1, 1188, 542, 4371, 4, 136, 2392, 1469, 3084, 2, 115, 1681, 38, 3082, 3083, 27, 1188, 434, 2392, 400, 4372, 4373, 458, 4374, 4375, 2, 113, 95, 1682, 126, 26, 342, 8, 445, 41, 261, 3, 1188, 542, 29, 13, 1958, 1470, 40, 1, 775, 111, 31, 977, 127, 232, 285, 777, 6, 273, 829, 67, 532, 10, 406, 3, 4376, 403, 285, 777, 6, 5, 900, 141, 1, 775, 1188, 542, 1959, 1, 285, 565, 64, 10, 43, 1683, 4377, 9, 1, 775, 51, 1, 978, 3085, 7, 18, 4378, 1, 634, 546, 6, 1, 164, 4, 532, 6, 1, 904, 2393, 95, 1, 974, 5, 4379, 4, 251, 1463, 763, 3086, 4, 1165, 2, 251, 774, 581, 3, 4380, 4381, 1071, 4382, 4, 3087, 899, 4383, 4384, 16, 33, 1, 1036, 4385, 543, 65, 193, 28, 37, 170, 4386, 1896, 6, 532, 273, 679, 7, 1461, 94, 403, 285, 6, 273, 2, 4387, 2, 3088, 15, 9, 1, 139, 402, 65, 10, 1, 237, 565, 632, 95, 170, 9, 3089, 4, 156, 273, 2, 251, 3090, 3, 100, 273, 3091, 292, 17, 832, 6, 546, 2, 518, 6, 4388, 4389, 1189, 9, 273, 131, 4, 532, 298, 1, 1188, 542, 9, 3092, 3, 1907, 43, 2394, 729, 1, 1075, 33, 67, 3, 1440, 1, 1036, 4, 273, 285, 249, 7, 57, 6, 532, 7, 1434, 532, 131, 31, 1185, 5, 2395, 831, 2, 1076, 546, 581, 3, 3026, 4390, 774, 2, 98, 1, 276, 19, 1, 113, 3093, 774, 9, 1, 285, 1189, 4, 273, 682, 3, 782, 1, 2344, 27, 1188, 542, 67, 3, 1440, 1, 4391, 512, 829, 70, 15, 10, 24, 3094, 286, 15, 128, 163, 3, 3095, 2396, 6, 1190, 2, 195, 128, 24, 119, 34, 3096, 2, 7, 14, 4392, 1684, 2397, 2, 24, 406, 3, 1960, 1, 465, 4, 1, 634, 285, 1188, 542, 80, 404, 74, 5, 683, 783, 1308, 783, 2, 215, 783, 2398, 2, 1685, 4, 86, 1933, 95, 2399, 2, 3066, 25, 4393, 1, 3097, 4394, 2, 113, 2398, 18, 4395, 1, 631, 2, 7, 189, 15, 33, 1310, 1, 2400, 285, 777, 373, 152, 532, 80, 1310, 1, 3098, 285, 777, 4, 402, 3, 403, 147, 1, 429, 543, 65, 8, 445, 84, 67, 15, 33, 596, 2, 37, 20, 1, 3093, 774, 9, 16, 292, 285, 6, 273, 9, 1, 429, 543, 65, 15, 10, 5, 146, 3099, 40, 1, 775, 51, 10, 1191, 3, 272, 158, 232, 285, 16, 31, 68, 977, 25, 1, 149, 1471, 399, 25, 1, 532, 131, 532, 131, 80, 4396, 24, 3, 143, 179, 227, 3, 195, 548, 3100, 232, 3101, 1686, 2345, 3, 273, 19, 3102, 402, 3103, 25, 4397, 1, 227, 1687, 4398, 17, 963, 164, 4, 1, 1926, 4399, 4400, 2, 4401, 1, 227, 1687, 266, 1961, 3104, 2, 634, 3104, 16, 597, 399, 17, 566, 5, 3105, 1311, 17, 4402, 4403, 3, 835, 1, 227, 2401, 2, 4404, 80, 1461, 94, 1462, 227, 2401, 2, 1076, 2402, 4, 227, 3, 195, 1688, 1456, 2995, 95, 1679, 19, 5, 3106, 1077, 162, 1, 164, 6, 5, 149, 64, 635, 684, 347, 1181, 265, 1, 1688, 1456, 4405, 581, 3, 1, 3107, 4, 4406, 4407, 158, 86, 191, 7, 359, 3108, 251, 4408, 18, 305, 43, 832, 6, 1, 251, 1463, 25, 232, 760, 1312, 730, 5, 1434, 4, 241, 65, 711, 147, 4409, 347, 4, 73, 3109, 95, 444, 125, 312, 4, 509, 31, 68, 444, 17, 273, 1689, 73, 1962, 3110, 1690, 7, 11, 29, 52, 15, 28, 37, 11, 82, 10, 5, 149, 1471, 1472, 162, 1, 51, 17, 539, 3, 273, 2, 11, 82, 5, 670, 218, 22, 31, 5, 318, 9, 439, 1, 422, 731, 836, 9, 1, 220, 284, 65, 63, 1, 108, 731, 836, 1691, 549, 1071, 3111, 276, 19, 12, 837, 358, 6, 1, 516, 4, 273, 2, 1180, 512, 1296, 64, 1424, 3, 1453, 6, 111, 9, 4410, 1, 285, 2, 832, 1, 546, 3, 272, 284, 285, 777, 6, 273, 64, 10, 1473, 596, 6, 111, 545, 9, 273, 291, 249, 662, 61, 720, 276, 19, 26, 358, 8, 56, 63, 3, 761, 1, 974, 115, 545, 9, 1, 273, 291, 5, 106, 4, 972, 1033, 2370, 491, 2, 4411, 103, 9, 135, 2, 1940, 195, 4, 1, 51, 763, 4412, 1, 1688, 1943, 4, 4413, 405, 1963, 176, 1181, 4414, 2, 4415, 405, 1964, 176, 1181, 2, 4416, 1065, 405, 1692, 176, 1181, 2, 761, 255, 4417, 276, 19, 1180, 1932, 2, 1165, 774, 3, 38, 1078, 4418, 1069, 4, 16, 405, 6, 1, 220, 284, 65, 1071, 3112, 1, 232, 176, 1181, 4, 251, 2403, 6, 4419, 176, 1181, 4, 405, 2404, 4, 4420, 4421, 4422, 251, 4423, 1313, 2, 4424, 6, 1451, 17, 4425, 175, 1164, 1905, 2405, 2, 761, 1454, 276, 19, 115, 9, 4426, 73, 251, 2403, 2, 1175, 1454, 9, 1156, 1, 2353, 633, 4427, 25, 302, 176, 1181, 25, 720, 899, 1, 3113, 4, 290, 4, 2406, 1669, 2, 1936, 10, 177, 3, 18, 3114, 1, 838, 251, 2, 98, 3115, 1, 4428, 6, 1, 972, 831, 59, 10, 5, 82, 3, 4429, 1, 1067, 6, 251, 2, 972, 831, 102, 115, 47, 7, 4430, 3116, 29, 21, 484, 9, 2407, 4431, 16, 4432, 64, 10, 905, 9, 3098, 122, 238, 4, 1, 51, 667, 6, 12, 4433, 633, 320, 175, 312, 9, 1965, 299, 833, 10, 235, 1, 1043, 567, 4, 1, 1965, 3117, 2, 3118, 4434, 1314, 5, 1050, 10, 406, 3, 784, 1, 432, 3119, 1966, 6, 255, 825, 4, 1, 320, 1693, 906, 16, 299, 10, 907, 3, 207, 3120, 4435, 6, 1, 1693, 64, 4436, 53, 75, 1, 432, 3119, 1966, 136, 3121, 27, 1, 1693, 1, 4437, 6, 64, 41, 31, 3, 124, 2, 1, 1967, 4, 1, 1966, 10, 4438, 9, 1, 299, 4, 1, 2408, 291, 18, 50, 17, 320, 175, 312, 9, 1965, 299, 833, 47, 7, 567, 10, 681, 4439, 2, 1694, 3, 32, 1, 2408, 3122, 4, 37, 956, 4, 4440, 35, 18, 1466, 59, 10, 5, 82, 3, 2390, 74, 115, 19, 67, 1, 135, 2, 1940, 195, 29, 114, 140, 102, 1, 1459, 4, 2350, 4441, 4442, 2, 1474, 245, 3123, 6, 34, 191, 47, 7, 534, 830, 625, 1180, 1296, 29, 21, 1192, 74, 64, 14, 163, 3, 488, 2409, 560, 2, 1055, 1, 3096, 2356, 721, 4, 1068, 4, 1, 896, 195, 9, 16, 598, 41, 39, 133, 415, 3123, 893, 776, 29, 21, 444, 25, 291, 6, 1, 3045, 304, 1434, 1695, 1, 2362, 6, 273, 4443, 195, 634, 291, 4444, 634, 4445, 3124, 3049, 251, 2, 1456, 103, 3125, 3043, 2, 4446, 1669, 2410, 2406, 2, 388, 1936, 2410, 972, 1315, 4447, 1696, 1968, 4448, 3126, 103, 2, 1079, 674, 4449, 2, 4450, 4451, 1696, 1687, 103, 2, 1, 3127, 4452, 27, 1, 2411, 2, 164, 2412, 1, 237, 4, 1, 149, 10, 2413, 4453, 19, 1, 4454, 534, 399, 417, 32, 1, 2362, 102, 4455, 534, 1969, 1697, 4456, 4457, 1180, 1296, 2, 1453, 291, 29, 1175, 5, 540, 3127, 3128, 9, 1439, 16, 785, 23, 43, 3129, 106, 149, 1475, 786, 74, 5, 3130, 4, 32, 1, 634, 206, 2, 115, 1949, 908, 3015, 6, 1, 669, 1180, 1932, 1920, 6, 1, 69, 25, 255, 951, 4458, 1696, 4459, 84, 19, 5, 4460, 2, 3131, 84, 3132, 966, 79, 8, 18, 305, 241, 254, 64, 18, 757, 599, 23, 318, 149, 2, 1970, 511, 582, 4, 2414, 320, 511, 115, 952, 1971, 582, 4, 2415, 1972, 115, 2, 106, 952, 2, 562, 724, 1080, 1316, 6, 245, 191, 881, 1, 175, 149, 4, 244, 73, 241, 1674, 1476, 6, 1, 948, 4, 76, 267, 2, 519, 8, 18, 404, 6, 32, 73, 241, 191, 8, 109, 3, 3133, 3, 13, 37, 8, 18, 1317, 19, 218, 27, 73, 241, 1674, 5, 287, 97, 18, 5, 318, 763, 287, 97, 18, 1081, 3, 579, 1, 318, 1071, 287, 97, 21, 406, 3, 909, 94, 43, 3134, 466, 899, 287, 97, 81, 67, 3, 1440, 5, 237, 2, 839, 667, 287, 97, 18, 423, 3, 123, 1698, 1466, 287, 88, 18, 1928, 6, 508, 1695, 287, 88, 21, 732, 6, 78, 399, 1475, 287, 97, 50, 17, 979, 2, 467, 17, 979, 9, 237, 6, 32, 55, 545, 13, 18, 3, 210, 670, 382, 670, 218, 452, 3135, 1, 318, 3, 89, 1, 1477, 459, 27, 1, 1973, 3, 1, 1974, 2416, 3, 3136, 27, 1699, 3, 3137, 2, 27, 41, 22, 1318, 539, 3, 41, 22, 3138, 540, 539, 8, 72, 319, 532, 85, 372, 10, 1082, 4461, 150, 382, 22, 14, 18, 1, 149, 4, 421, 320, 195, 638, 4, 34, 2953, 26, 118, 1061, 3, 32, 4, 13, 128, 194, 275, 13]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "print(docs[1], sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3000,\n",
       " 3185,\n",
       " 2956,\n",
       " 4123,\n",
       " 1201,\n",
       " 2950,\n",
       " 3153,\n",
       " 453,\n",
       " 3049,\n",
       " 289,\n",
       " 2510,\n",
       " 1025,\n",
       " 1981,\n",
       " 2398,\n",
       " 1877,\n",
       " 4676,\n",
       " 2484,\n",
       " 2382,\n",
       " 812,\n",
       " 2168,\n",
       " 1884,\n",
       " 1582,\n",
       " 1742,\n",
       " 830,\n",
       " 677,\n",
       " 2486,\n",
       " 2690,\n",
       " 3411,\n",
       " 2203,\n",
       " 2734,\n",
       " 2315,\n",
       " 4110,\n",
       " 1879,\n",
       " 279,\n",
       " 809,\n",
       " 1141]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(i) for i in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2532,\n",
       " 30,\n",
       " 2065,\n",
       " 6,\n",
       " 4786,\n",
       " 217,\n",
       " 257,\n",
       " 2487,\n",
       " 19,\n",
       " 1,\n",
       " 576,\n",
       " 768,\n",
       " 221,\n",
       " 127,\n",
       " 75,\n",
       " 385,\n",
       " 18,\n",
       " 814,\n",
       " 570,\n",
       " 1,\n",
       " 2066,\n",
       " 835,\n",
       " 4787,\n",
       " 2,\n",
       " 4788,\n",
       " 94,\n",
       " 1,\n",
       " 4789,\n",
       " 13,\n",
       " 14,\n",
       " 21,\n",
       " 822,\n",
       " 3,\n",
       " 311,\n",
       " 316,\n",
       " 13,\n",
       " 18,\n",
       " 153,\n",
       " 305,\n",
       " 193,\n",
       " 15,\n",
       " 14,\n",
       " 21,\n",
       " 5,\n",
       " 376,\n",
       " 463,\n",
       " 28,\n",
       " 52,\n",
       " 24,\n",
       " 3315,\n",
       " 15,\n",
       " 2067,\n",
       " 15,\n",
       " 62,\n",
       " 2961,\n",
       " 32,\n",
       " 236,\n",
       " 15,\n",
       " 41,\n",
       " 734,\n",
       " 14,\n",
       " 163,\n",
       " 3,\n",
       " 250,\n",
       " 2,\n",
       " 193,\n",
       " 13,\n",
       " 81,\n",
       " 15,\n",
       " 13,\n",
       " 14,\n",
       " 18,\n",
       " 316,\n",
       " 2068,\n",
       " 486,\n",
       " 53,\n",
       " 40,\n",
       " 924,\n",
       " 3316,\n",
       " 5,\n",
       " 762,\n",
       " 909,\n",
       " 23,\n",
       " 43,\n",
       " 358,\n",
       " 39,\n",
       " 5,\n",
       " 3317,\n",
       " 5,\n",
       " 146,\n",
       " 713,\n",
       " 1902,\n",
       " 249,\n",
       " 49,\n",
       " 762,\n",
       " 909,\n",
       " 27,\n",
       " 157,\n",
       " 3318,\n",
       " 3,\n",
       " 111,\n",
       " 30,\n",
       " 33,\n",
       " 3319,\n",
       " 19,\n",
       " 224,\n",
       " 1,\n",
       " 1525,\n",
       " 75,\n",
       " 1,\n",
       " 1162,\n",
       " 2,\n",
       " 762,\n",
       " 335,\n",
       " 1347,\n",
       " 714,\n",
       " 49,\n",
       " 115,\n",
       " 1461,\n",
       " 6,\n",
       " 1,\n",
       " 1329,\n",
       " 4,\n",
       " 1903,\n",
       " 4,\n",
       " 538,\n",
       " 4,\n",
       " 244,\n",
       " 885,\n",
       " 1041,\n",
       " 622,\n",
       " 33,\n",
       " 3320,\n",
       " 845,\n",
       " 2069,\n",
       " 52,\n",
       " 13,\n",
       " 81,\n",
       " 43,\n",
       " 320,\n",
       " 766,\n",
       " 22,\n",
       " 248,\n",
       " 24,\n",
       " 18,\n",
       " 2045,\n",
       " 419,\n",
       " 151,\n",
       " 28,\n",
       " 80,\n",
       " 3277,\n",
       " 386,\n",
       " 2,\n",
       " 262,\n",
       " 9,\n",
       " 958,\n",
       " 64,\n",
       " 446,\n",
       " 205,\n",
       " 3,\n",
       " 1052,\n",
       " 3,\n",
       " 1,\n",
       " 3278,\n",
       " 2046,\n",
       " 4,\n",
       " 2417,\n",
       " 115,\n",
       " 165,\n",
       " 4,\n",
       " 64,\n",
       " 20,\n",
       " 200,\n",
       " 265,\n",
       " 1328,\n",
       " 593,\n",
       " 2,\n",
       " 3279,\n",
       " 32,\n",
       " 908,\n",
       " 58,\n",
       " 4790,\n",
       " 632,\n",
       " 3,\n",
       " 784,\n",
       " 2045,\n",
       " 3280,\n",
       " 30,\n",
       " 33,\n",
       " 5,\n",
       " 146,\n",
       " 320,\n",
       " 1737,\n",
       " 22,\n",
       " 174,\n",
       " 3281,\n",
       " 1,\n",
       " 461,\n",
       " 4,\n",
       " 1,\n",
       " 168,\n",
       " 3282,\n",
       " 2,\n",
       " 2533,\n",
       " 2534,\n",
       " 766,\n",
       " 400,\n",
       " 1695,\n",
       " 1475,\n",
       " 2047,\n",
       " 6,\n",
       " 1338,\n",
       " 15,\n",
       " 10,\n",
       " 24,\n",
       " 43,\n",
       " 3283,\n",
       " 3,\n",
       " 171,\n",
       " 7,\n",
       " 15,\n",
       " 33,\n",
       " 400,\n",
       " 2047,\n",
       " 22,\n",
       " 884,\n",
       " 5,\n",
       " 90,\n",
       " 766,\n",
       " 9,\n",
       " 1,\n",
       " 58,\n",
       " 16,\n",
       " 766,\n",
       " 33,\n",
       " 4,\n",
       " 244,\n",
       " 2495,\n",
       " 2496,\n",
       " 9,\n",
       " 922,\n",
       " 78,\n",
       " 506,\n",
       " 33,\n",
       " 5,\n",
       " 2315,\n",
       " 3284,\n",
       " 52,\n",
       " 13,\n",
       " 81,\n",
       " 53,\n",
       " 5,\n",
       " 256,\n",
       " 4,\n",
       " 44,\n",
       " 2,\n",
       " 53,\n",
       " 5,\n",
       " 85,\n",
       " 3321,\n",
       " 2070,\n",
       " 3,\n",
       " 356,\n",
       " 815,\n",
       " 2,\n",
       " 156,\n",
       " 115,\n",
       " 49,\n",
       " 168,\n",
       " 954,\n",
       " 237,\n",
       " 33,\n",
       " 1,\n",
       " 4791,\n",
       " 1643,\n",
       " 1040,\n",
       " 1,\n",
       " 3322,\n",
       " 1040,\n",
       " 3323,\n",
       " 1,\n",
       " 3324,\n",
       " 1100,\n",
       " 494,\n",
       " 77,\n",
       " 264,\n",
       " 3325,\n",
       " 642,\n",
       " 1100,\n",
       " 4,\n",
       " 5,\n",
       " 524,\n",
       " 3326,\n",
       " 1111,\n",
       " 39,\n",
       " 3327,\n",
       " 1,\n",
       " 1204,\n",
       " 1100,\n",
       " 9,\n",
       " 64,\n",
       " 5,\n",
       " 1111,\n",
       " 14,\n",
       " 1526,\n",
       " 3328,\n",
       " 94,\n",
       " 5,\n",
       " 3329,\n",
       " 1111,\n",
       " 39,\n",
       " 817,\n",
       " 1901,\n",
       " 974,\n",
       " 5,\n",
       " 3330,\n",
       " 1,\n",
       " 1040,\n",
       " 33,\n",
       " 108,\n",
       " 1441,\n",
       " 25,\n",
       " 5,\n",
       " 600,\n",
       " 253,\n",
       " 19,\n",
       " 5,\n",
       " 1756,\n",
       " 27,\n",
       " 111,\n",
       " 3,\n",
       " 2534,\n",
       " 4792,\n",
       " 471,\n",
       " 8,\n",
       " 72,\n",
       " 4793,\n",
       " 3,\n",
       " 400,\n",
       " 3111,\n",
       " 1643,\n",
       " 22,\n",
       " 692,\n",
       " 49,\n",
       " 775,\n",
       " 85,\n",
       " 9,\n",
       " 115,\n",
       " 115,\n",
       " 2,\n",
       " 115,\n",
       " 19,\n",
       " 4794,\n",
       " 79,\n",
       " 59,\n",
       " 33,\n",
       " 5,\n",
       " 90,\n",
       " 156,\n",
       " 923,\n",
       " 22,\n",
       " 10,\n",
       " 601,\n",
       " 9,\n",
       " 1730,\n",
       " 1513,\n",
       " 99,\n",
       " 1105,\n",
       " 24,\n",
       " 41,\n",
       " 28,\n",
       " 133,\n",
       " 845,\n",
       " 2535,\n",
       " 41,\n",
       " 9,\n",
       " 583,\n",
       " 2,\n",
       " 250,\n",
       " 9,\n",
       " 2049,\n",
       " 22,\n",
       " 10,\n",
       " 99,\n",
       " 99,\n",
       " 10,\n",
       " 1514,\n",
       " 1515,\n",
       " 1514,\n",
       " 1515,\n",
       " 884,\n",
       " 1513,\n",
       " 2,\n",
       " 99,\n",
       " 33,\n",
       " 346,\n",
       " 115,\n",
       " 19,\n",
       " 1,\n",
       " 1163,\n",
       " 4,\n",
       " 1516,\n",
       " 19,\n",
       " 159,\n",
       " 103,\n",
       " 1,\n",
       " 178,\n",
       " 1516,\n",
       " 64,\n",
       " 99,\n",
       " 884,\n",
       " 787,\n",
       " 105,\n",
       " 2,\n",
       " 99,\n",
       " 1339,\n",
       " 105,\n",
       " 85,\n",
       " 9,\n",
       " 987,\n",
       " 1,\n",
       " 1517,\n",
       " 4,\n",
       " 159,\n",
       " 85,\n",
       " 79,\n",
       " 8,\n",
       " 18,\n",
       " 47,\n",
       " 314,\n",
       " 246,\n",
       " 577,\n",
       " 176,\n",
       " 366,\n",
       " 6,\n",
       " 111,\n",
       " 2,\n",
       " 1287,\n",
       " 6,\n",
       " 5,\n",
       " 709,\n",
       " 198,\n",
       " 57,\n",
       " 8,\n",
       " 1317,\n",
       " 78,\n",
       " 366,\n",
       " 743,\n",
       " 3,\n",
       " 21,\n",
       " 146,\n",
       " 7,\n",
       " 10,\n",
       " 13,\n",
       " 28,\n",
       " 1,\n",
       " 58,\n",
       " 32,\n",
       " 236,\n",
       " 13,\n",
       " 10,\n",
       " 346,\n",
       " 136,\n",
       " 118,\n",
       " 120,\n",
       " 2,\n",
       " 424,\n",
       " 3,\n",
       " 100,\n",
       " 13,\n",
       " 83,\n",
       " 924,\n",
       " 523,\n",
       " 1,\n",
       " 965,\n",
       " 26,\n",
       " 104,\n",
       " 79,\n",
       " 10,\n",
       " 7,\n",
       " 13,\n",
       " 18,\n",
       " 3,\n",
       " 339,\n",
       " 1,\n",
       " 1739,\n",
       " 1340,\n",
       " 64,\n",
       " 173,\n",
       " 159,\n",
       " 282,\n",
       " 29,\n",
       " 268,\n",
       " 1103,\n",
       " 3,\n",
       " 339,\n",
       " 2,\n",
       " 153,\n",
       " 646,\n",
       " 794,\n",
       " 1518,\n",
       " 13,\n",
       " 1740,\n",
       " 38,\n",
       " 55,\n",
       " 2536,\n",
       " 252,\n",
       " 7,\n",
       " 10,\n",
       " 5,\n",
       " 146,\n",
       " 13,\n",
       " 79,\n",
       " 37,\n",
       " 14,\n",
       " 21,\n",
       " 55,\n",
       " 770,\n",
       " 3,\n",
       " 339,\n",
       " 16,\n",
       " 1340,\n",
       " 4795,\n",
       " 9,\n",
       " 1214,\n",
       " 9,\n",
       " 366,\n",
       " 67,\n",
       " 416,\n",
       " 1214,\n",
       " 114,\n",
       " 59,\n",
       " 20,\n",
       " 125,\n",
       " 1210,\n",
       " 1341,\n",
       " 594,\n",
       " 43,\n",
       " 975,\n",
       " 6,\n",
       " 85,\n",
       " 193,\n",
       " 684,\n",
       " 65,\n",
       " 4,\n",
       " 737,\n",
       " 2050,\n",
       " 203,\n",
       " 1177,\n",
       " 226,\n",
       " 50,\n",
       " 426,\n",
       " 1,\n",
       " 975,\n",
       " 2,\n",
       " 1519,\n",
       " 3,\n",
       " 608,\n",
       " 1,\n",
       " 449,\n",
       " 2,\n",
       " 467,\n",
       " 96,\n",
       " 13,\n",
       " 81,\n",
       " 67,\n",
       " 1,\n",
       " 146,\n",
       " 1655,\n",
       " 343,\n",
       " 2323,\n",
       " 2,\n",
       " 98,\n",
       " 13,\n",
       " 579,\n",
       " 67,\n",
       " 3,\n",
       " 759,\n",
       " 644,\n",
       " 94,\n",
       " 146,\n",
       " 13,\n",
       " 2,\n",
       " 585,\n",
       " 32,\n",
       " 37,\n",
       " 13,\n",
       " 82,\n",
       " 5,\n",
       " 14,\n",
       " 227,\n",
       " 2,\n",
       " 409,\n",
       " 7,\n",
       " 13,\n",
       " 29,\n",
       " 272,\n",
       " 90,\n",
       " 3331,\n",
       " 6,\n",
       " 16,\n",
       " 2556,\n",
       " 112,\n",
       " 54,\n",
       " 1198,\n",
       " 954,\n",
       " 3332,\n",
       " 4,\n",
       " 2557,\n",
       " 296,\n",
       " 2558,\n",
       " 2559,\n",
       " 2560,\n",
       " 2561,\n",
       " 2562,\n",
       " 689,\n",
       " 3,\n",
       " 307,\n",
       " 13,\n",
       " 95,\n",
       " 181,\n",
       " 17,\n",
       " 432,\n",
       " 13,\n",
       " 95,\n",
       " 181,\n",
       " 17,\n",
       " 1051,\n",
       " 2,\n",
       " 740,\n",
       " 13,\n",
       " 95,\n",
       " 181,\n",
       " 17,\n",
       " 522,\n",
       " 2,\n",
       " 254,\n",
       " 13,\n",
       " 95,\n",
       " 181,\n",
       " 17,\n",
       " 990,\n",
       " 13,\n",
       " 95,\n",
       " 181,\n",
       " 17,\n",
       " 689,\n",
       " 13,\n",
       " 20,\n",
       " 24,\n",
       " 1510,\n",
       " 9,\n",
       " 2034,\n",
       " 47,\n",
       " 2563,\n",
       " 1166,\n",
       " 13,\n",
       " 18,\n",
       " 689,\n",
       " 671,\n",
       " 3,\n",
       " 290,\n",
       " 84,\n",
       " 3,\n",
       " 307,\n",
       " 2561,\n",
       " 2562,\n",
       " 2557,\n",
       " 296,\n",
       " 2558,\n",
       " 2559,\n",
       " 2560,\n",
       " 26,\n",
       " 629,\n",
       " 3,\n",
       " 13,\n",
       " 104,\n",
       " 79,\n",
       " 10,\n",
       " 7,\n",
       " 151,\n",
       " 1521,\n",
       " 13,\n",
       " 689,\n",
       " 3,\n",
       " 307,\n",
       " 1214,\n",
       " 425,\n",
       " 74,\n",
       " 4,\n",
       " 1215,\n",
       " 6,\n",
       " 12,\n",
       " 1979,\n",
       " 2492,\n",
       " 535,\n",
       " 7,\n",
       " 8,\n",
       " 14,\n",
       " 609,\n",
       " 47,\n",
       " 207,\n",
       " 41,\n",
       " 4,\n",
       " 13,\n",
       " 1757,\n",
       " 145,\n",
       " 2,\n",
       " 2430,\n",
       " 14,\n",
       " 18,\n",
       " 689,\n",
       " 4,\n",
       " 1215,\n",
       " 1,\n",
       " 689,\n",
       " 4,\n",
       " 1215,\n",
       " 14,\n",
       " 821,\n",
       " 163,\n",
       " 3,\n",
       " 203,\n",
       " 64,\n",
       " 14,\n",
       " 100,\n",
       " 13,\n",
       " 3,\n",
       " 307,\n",
       " 23,\n",
       " 5,\n",
       " 1758,\n",
       " 39,\n",
       " 43,\n",
       " 2564,\n",
       " 39,\n",
       " 5,\n",
       " 600,\n",
       " 39,\n",
       " 5,\n",
       " 483,\n",
       " 39,\n",
       " 5,\n",
       " 462,\n",
       " 287,\n",
       " 39,\n",
       " 5,\n",
       " 3333,\n",
       " 39,\n",
       " 5,\n",
       " 3334,\n",
       " 39,\n",
       " 693,\n",
       " 13,\n",
       " 109,\n",
       " 3,\n",
       " 21,\n",
       " 966,\n",
       " 348,\n",
       " 8,\n",
       " 56,\n",
       " 63,\n",
       " 3,\n",
       " 384,\n",
       " 13,\n",
       " 37,\n",
       " 56,\n",
       " 13,\n",
       " 63,\n",
       " 3,\n",
       " 21,\n",
       " 349,\n",
       " 9,\n",
       " 13,\n",
       " 18,\n",
       " 3,\n",
       " 1331,\n",
       " 644,\n",
       " 2,\n",
       " 599,\n",
       " 55,\n",
       " 85,\n",
       " 13,\n",
       " 88,\n",
       " 1332,\n",
       " 15,\n",
       " 19,\n",
       " 5,\n",
       " 209,\n",
       " 7,\n",
       " 209,\n",
       " 128,\n",
       " 21,\n",
       " 5,\n",
       " 197,\n",
       " 309,\n",
       " 209,\n",
       " 6,\n",
       " 1,\n",
       " 1099,\n",
       " 4,\n",
       " 159,\n",
       " 234,\n",
       " 2,\n",
       " 13,\n",
       " 14,\n",
       " 21,\n",
       " 349,\n",
       " 9,\n",
       " 566,\n",
       " 7,\n",
       " 41,\n",
       " 209,\n",
       " 6,\n",
       " 1,\n",
       " 234,\n",
       " 4,\n",
       " 1,\n",
       " 51,\n",
       " 408,\n",
       " 7,\n",
       " 209,\n",
       " 10,\n",
       " 1,\n",
       " 209,\n",
       " 4,\n",
       " 985,\n",
       " 1,\n",
       " 209,\n",
       " 4,\n",
       " 356,\n",
       " 39,\n",
       " 1,\n",
       " 209,\n",
       " 4,\n",
       " 734,\n",
       " 39,\n",
       " 1,\n",
       " 209,\n",
       " 4,\n",
       " 544,\n",
       " 2,\n",
       " 4796,\n",
       " 5,\n",
       " 905,\n",
       " 103,\n",
       " 2,\n",
       " 560,\n",
       " 9,\n",
       " 1,\n",
       " 51,\n",
       " 428,\n",
       " 300,\n",
       " 1523,\n",
       " 373,\n",
       " 13,\n",
       " 20,\n",
       " 32,\n",
       " 6,\n",
       " 1,\n",
       " 149,\n",
       " 4,\n",
       " 710,\n",
       " 44,\n",
       " 3,\n",
       " 1,\n",
       " 87,\n",
       " 8,\n",
       " 56,\n",
       " 63,\n",
       " 3,\n",
       " 1198,\n",
       " 5,\n",
       " 1201,\n",
       " 4797,\n",
       " 629,\n",
       " 3,\n",
       " 32,\n",
       " 4,\n",
       " 42,\n",
       " 25,\n",
       " 4798,\n",
       " 4799,\n",
       " 4800,\n",
       " 65,\n",
       " 410,\n",
       " 40,\n",
       " 13,\n",
       " 20,\n",
       " 1759,\n",
       " 25,\n",
       " 165,\n",
       " 90,\n",
       " 598,\n",
       " 165,\n",
       " 1527,\n",
       " 680,\n",
       " 32,\n",
       " 55,\n",
       " 678,\n",
       " 691,\n",
       " 34,\n",
       " 4801,\n",
       " 55,\n",
       " 535,\n",
       " 4802,\n",
       " 2521,\n",
       " 55,\n",
       " 4803,\n",
       " 4804,\n",
       " 6,\n",
       " 78,\n",
       " 1336,\n",
       " 2,\n",
       " 13,\n",
       " 311,\n",
       " 644,\n",
       " 6,\n",
       " 5,\n",
       " 60,\n",
       " 90,\n",
       " 2,\n",
       " 2565,\n",
       " 58,\n",
       " 4805,\n",
       " 795,\n",
       " 3084,\n",
       " 2,\n",
       " 4806,\n",
       " 114,\n",
       " 1528,\n",
       " 2,\n",
       " 13,\n",
       " 1760,\n",
       " 644,\n",
       " 3,\n",
       " 21,\n",
       " 5,\n",
       " 494,\n",
       " 713,\n",
       " 25,\n",
       " 314,\n",
       " 77,\n",
       " 13,\n",
       " 268,\n",
       " 4807,\n",
       " 644,\n",
       " 3,\n",
       " 21,\n",
       " 17,\n",
       " 73,\n",
       " 448,\n",
       " 112,\n",
       " 54,\n",
       " 1637,\n",
       " 32,\n",
       " 4,\n",
       " 13,\n",
       " 237,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 4,\n",
       " 2990,\n",
       " 235,\n",
       " 55,\n",
       " 146,\n",
       " 1112,\n",
       " 1348,\n",
       " 6,\n",
       " 44,\n",
       " 25,\n",
       " 1439,\n",
       " 820,\n",
       " 4,\n",
       " 509,\n",
       " 3,\n",
       " 824,\n",
       " 1,\n",
       " 87,\n",
       " 3,\n",
       " 336,\n",
       " 1,\n",
       " 44,\n",
       " 92,\n",
       " 9,\n",
       " 1636,\n",
       " 34,\n",
       " 151,\n",
       " 2,\n",
       " 115,\n",
       " 3335,\n",
       " 417,\n",
       " 1,\n",
       " 271,\n",
       " 87,\n",
       " 128,\n",
       " 194,\n",
       " 275,\n",
       " 13]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[4][-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 1000. Again, we can do this with a built in Keras's pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (36, 1000)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2532,   30, 2065,    6, 4786,  217,  257, 2487,   19,    1,  576,\n",
       "        768,  221,  127,   75,  385,   18,  814,  570,    1, 2066,  835,\n",
       "       4787,    2, 4788,   94,    1, 4789,   13,   14,   21,  822,    3,\n",
       "        311,  316,   13,   18,  153,  305,  193,   15,   14,   21,    5,\n",
       "        376,  463,   28,   52,   24, 3315,   15, 2067,   15,   62, 2961,\n",
       "         32,  236,   15,   41,  734,   14,  163,    3,  250,    2,  193,\n",
       "         13,   81,   15,   13,   14,   18,  316, 2068,  486,   53,   40,\n",
       "        924, 3316,    5,  762,  909,   23,   43,  358,   39,    5, 3317,\n",
       "          5,  146,  713, 1902,  249,   49,  762,  909,   27,  157, 3318,\n",
       "          3,  111,   30,   33, 3319,   19,  224,    1, 1525,   75,    1,\n",
       "       1162,    2,  762,  335, 1347,  714,   49,  115, 1461,    6,    1,\n",
       "       1329,    4, 1903,    4,  538,    4,  244,  885, 1041,  622,   33,\n",
       "       3320,  845, 2069,   52,   13,   81,   43,  320,  766,   22,  248,\n",
       "         24,   18, 2045,  419,  151,   28,   80, 3277,  386,    2,  262,\n",
       "          9,  958,   64,  446,  205,    3, 1052,    3,    1, 3278, 2046,\n",
       "          4, 2417,  115,  165,    4,   64,   20,  200,  265, 1328,  593,\n",
       "          2, 3279,   32,  908,   58, 4790,  632,    3,  784, 2045, 3280,\n",
       "         30,   33,    5,  146,  320, 1737,   22,  174, 3281,    1,  461,\n",
       "          4,    1,  168, 3282,    2, 2533, 2534,  766,  400, 1695, 1475,\n",
       "       2047,    6, 1338,   15,   10,   24,   43, 3283,    3,  171,    7,\n",
       "         15,   33,  400, 2047,   22,  884,    5,   90,  766,    9,    1,\n",
       "         58,   16,  766,   33,    4,  244, 2495, 2496,    9,  922,   78,\n",
       "        506,   33,    5, 2315, 3284,   52,   13,   81,   53,    5,  256,\n",
       "          4,   44,    2,   53,    5,   85, 3321, 2070,    3,  356,  815,\n",
       "          2,  156,  115,   49,  168,  954,  237,   33,    1, 4791, 1643,\n",
       "       1040,    1, 3322, 1040, 3323,    1, 3324, 1100,  494,   77,  264,\n",
       "       3325,  642, 1100,    4,    5,  524, 3326, 1111,   39, 3327,    1,\n",
       "       1204, 1100,    9,   64,    5, 1111,   14, 1526, 3328,   94,    5,\n",
       "       3329, 1111,   39,  817, 1901,  974,    5, 3330,    1, 1040,   33,\n",
       "        108, 1441,   25,    5,  600,  253,   19,    5, 1756,   27,  111,\n",
       "          3, 2534, 4792,  471,    8,   72, 4793,    3,  400, 3111, 1643,\n",
       "         22,  692,   49,  775,   85,    9,  115,  115,    2,  115,   19,\n",
       "       4794,   79,   59,   33,    5,   90,  156,  923,   22,   10,  601,\n",
       "          9, 1730, 1513,   99, 1105,   24,   41,   28,  133,  845, 2535,\n",
       "         41,    9,  583,    2,  250,    9, 2049,   22,   10,   99,   99,\n",
       "         10, 1514, 1515, 1514, 1515,  884, 1513,    2,   99,   33,  346,\n",
       "        115,   19,    1, 1163,    4, 1516,   19,  159,  103,    1,  178,\n",
       "       1516,   64,   99,  884,  787,  105,    2,   99, 1339,  105,   85,\n",
       "          9,  987,    1, 1517,    4,  159,   85,   79,    8,   18,   47,\n",
       "        314,  246,  577,  176,  366,    6,  111,    2, 1287,    6,    5,\n",
       "        709,  198,   57,    8, 1317,   78,  366,  743,    3,   21,  146,\n",
       "          7,   10,   13,   28,    1,   58,   32,  236,   13,   10,  346,\n",
       "        136,  118,  120,    2,  424,    3,  100,   13,   83,  924,  523,\n",
       "          1,  965,   26,  104,   79,   10,    7,   13,   18,    3,  339,\n",
       "          1, 1739, 1340,   64,  173,  159,  282,   29,  268, 1103,    3,\n",
       "        339,    2,  153,  646,  794, 1518,   13, 1740,   38,   55, 2536,\n",
       "        252,    7,   10,    5,  146,   13,   79,   37,   14,   21,   55,\n",
       "        770,    3,  339,   16, 1340, 4795,    9, 1214,    9,  366,   67,\n",
       "        416, 1214,  114,   59,   20,  125, 1210, 1341,  594,   43,  975,\n",
       "          6,   85,  193,  684,   65,    4,  737, 2050,  203, 1177,  226,\n",
       "         50,  426,    1,  975,    2, 1519,    3,  608,    1,  449,    2,\n",
       "        467,   96,   13,   81,   67,    1,  146, 1655,  343, 2323,    2,\n",
       "         98,   13,  579,   67,    3,  759,  644,   94,  146,   13,    2,\n",
       "        585,   32,   37,   13,   82,    5,   14,  227,    2,  409,    7,\n",
       "         13,   29,  272,   90, 3331,    6,   16, 2556,  112,   54, 1198,\n",
       "        954, 3332,    4, 2557,  296, 2558, 2559, 2560, 2561, 2562,  689,\n",
       "          3,  307,   13,   95,  181,   17,  432,   13,   95,  181,   17,\n",
       "       1051,    2,  740,   13,   95,  181,   17,  522,    2,  254,   13,\n",
       "         95,  181,   17,  990,   13,   95,  181,   17,  689,   13,   20,\n",
       "         24, 1510,    9, 2034,   47, 2563, 1166,   13,   18,  689,  671,\n",
       "          3,  290,   84,    3,  307, 2561, 2562, 2557,  296, 2558, 2559,\n",
       "       2560,   26,  629,    3,   13,  104,   79,   10,    7,  151, 1521,\n",
       "         13,  689,    3,  307, 1214,  425,   74,    4, 1215,    6,   12,\n",
       "       1979, 2492,  535,    7,    8,   14,  609,   47,  207,   41,    4,\n",
       "         13, 1757,  145,    2, 2430,   14,   18,  689,    4, 1215,    1,\n",
       "        689,    4, 1215,   14,  821,  163,    3,  203,   64,   14,  100,\n",
       "         13,    3,  307,   23,    5, 1758,   39,   43, 2564,   39,    5,\n",
       "        600,   39,    5,  483,   39,    5,  462,  287,   39,    5, 3333,\n",
       "         39,    5, 3334,   39,  693,   13,  109,    3,   21,  966,  348,\n",
       "          8,   56,   63,    3,  384,   13,   37,   56,   13,   63,    3,\n",
       "         21,  349,    9,   13,   18,    3, 1331,  644,    2,  599,   55,\n",
       "         85,   13,   88, 1332,   15,   19,    5,  209,    7,  209,  128,\n",
       "         21,    5,  197,  309,  209,    6,    1, 1099,    4,  159,  234,\n",
       "          2,   13,   14,   21,  349,    9,  566,    7,   41,  209,    6,\n",
       "          1,  234,    4,    1,   51,  408,    7,  209,   10,    1,  209,\n",
       "          4,  985,    1,  209,    4,  356,   39,    1,  209,    4,  734,\n",
       "         39,    1,  209,    4,  544,    2, 4796,    5,  905,  103,    2,\n",
       "        560,    9,    1,   51,  428,  300, 1523,  373,   13,   20,   32,\n",
       "          6,    1,  149,    4,  710,   44,    3,    1,   87,    8,   56,\n",
       "         63,    3, 1198,    5, 1201, 4797,  629,    3,   32,    4,   42,\n",
       "         25, 4798, 4799, 4800,   65,  410,   40,   13,   20, 1759,   25,\n",
       "        165,   90,  598,  165, 1527,  680,   32,   55,  678,  691,   34,\n",
       "       4801,   55,  535, 4802, 2521,   55, 4803, 4804,    6,   78, 1336,\n",
       "          2,   13,  311,  644,    6,    5,   60,   90,    2, 2565,   58,\n",
       "       4805,  795, 3084,    2, 4806,  114, 1528,    2,   13, 1760,  644,\n",
       "          3,   21,    5,  494,  713,   25,  314,   77,   13,  268, 4807,\n",
       "        644,    3,   21,   17,   73,  448,  112,   54, 1637,   32,    4,\n",
       "         13,  237,    6,   34,  149,    4, 2990,  235,   55,  146, 1112,\n",
       "       1348,    6,   44,   25, 1439,  820,    4,  509,    3,  824,    1,\n",
       "         87,    3,  336,    1,   44,   92,    9, 1636,   34,  151,    2,\n",
       "        115, 3335,  417,    1,  271,   87,  128,  194,  275,   13])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a test set\n",
    "X_train = np.append(data[:10], data[12:22], axis=0)\n",
    "X_train = np.append(X_train, data[24:34], axis=0)\n",
    "\n",
    "X_test = np.append(data[10:12], data[22:24], axis=0)\n",
    "X_test = np.append(X_test, data[34:], axis=0)\n",
    "\n",
    "y_train = np.append(labels[:10], labels[12:22], axis=0)\n",
    "y_train = np.append(y_train, labels[24:34], axis=0)\n",
    "\n",
    "y_test = np.append(labels[10:12], labels[22:24],axis=0)\n",
    "y_test = np.append(y_test, labels[34:],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparing the Embedding layer\n",
    "\n",
    "Compute an index mapping words to known embeddings, by parsing the data dump of pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(PATH, 'glove.6B.50d.txt'), encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coeffs=np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word]=coeffs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7314, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_Matrix = np.zeros((vocab_Size, 50))\n",
    "\n",
    "for word,i in word_Index.items():\n",
    "    embedding_Vector=embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_Vector is not None:\n",
    "        embedding_Matrix[i] = embedding_Vector\n",
    "    \n",
    "\n",
    "print (embedding_Matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create the embedding layer\n",
    "\n",
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. \n",
    "\n",
    "    We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. \n",
    "    We do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally build a small 1D convnet to solve our classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 00:40:48.681789 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0814 00:40:48.801378 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0814 00:40:48.829478 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0814 00:40:48.877720 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0814 00:40:48.881686 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0814 00:40:49.425283 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32')\n",
    "\n",
    "embedded_sequences=Embedding(vocab_Size,50, weights=[embedding_Matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)(sequence_input)\n",
    "\n",
    "\n",
    "\n",
    "x = Conv1D(64,5,activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "\n",
    "x = Conv1D(64,5,activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(4)(x)\n",
    "\n",
    "x =Flatten()(x)\n",
    "\n",
    "x = Dense(64,activation='relu')(x)\n",
    "\n",
    "preds = Dense(len(labels_Index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 00:40:54.149329 17264 deprecation_wrapper.py:119] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          365700    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 996, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 15936)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1019968   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,401,927\n",
      "Trainable params: 1,036,227\n",
      "Non-trainable params: 365,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0814 00:41:01.321147 17264 deprecation.py:323] From C:\\Users\\Arunabh\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 1.2363 - acc: 0.3667\n",
      "Epoch 2/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.5475 - acc: 0.3333\n",
      "Epoch 3/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 9.3563 - acc: 0.3333\n",
      "Epoch 4/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 5/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 6/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 7/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 8/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 9/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 10/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 11/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 12/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 13/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 14/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 15/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 16/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 17/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 18/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 19/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 20/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 21/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 22/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 23/25\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 24/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n",
      "Epoch 25/25\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 10.7454 - acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24171311400>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8536727e-29 3.0647894e-32 1.0000000e+00]\n",
      " [1.4419088e-28 1.2909744e-31 1.0000000e+00]\n",
      " [3.7601278e-27 2.2561594e-29 1.0000000e+00]\n",
      " [1.0396522e-24 6.4409268e-27 1.0000000e+00]\n",
      " [2.6095822e-23 2.5974631e-25 1.0000000e+00]\n",
      " [3.7091261e-27 2.0602837e-29 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred =[]\n",
    "for i in Y_pred:\n",
    "    y_pred.append(np.argmax(i))\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2],\n",
       "       [0, 0, 2],\n",
       "       [0, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "\n",
    "Dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
    "\n",
    "Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm=LSTM(200)(embedded_sequences)\n",
    "\n",
    "preds=Dense(1,activation='sigmoid')(lstm)\n",
    "\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1000, 50)          365700    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               200800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 566,701\n",
      "Trainable params: 201,001\n",
      "Non-trainable params: 365,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "30/30 [==============================] - 15s 497ms/step - loss: 0.9134 - acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f4e802be0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 1, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.67%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "    \n",
    "    https://keras.io\n",
    "        \n",
    "    https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
